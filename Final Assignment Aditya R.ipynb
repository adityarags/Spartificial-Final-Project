{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> AI4LR - Final Assignment [100 Points]","metadata":{"id":"xksEYwr8QXgN"}},{"cell_type":"code","source":"# IMPORTANT\nYour_name = \"Aditya R\"\nYour_emailid = \"adiramachandran59@gmail.com\"","metadata":{"id":"2WVqrOVwm8yz","execution":{"iopub.status.busy":"2022-11-20T15:09:09.046763Z","iopub.execute_input":"2022-11-20T15:09:09.047223Z","iopub.status.idle":"2022-11-20T15:09:09.052140Z","shell.execute_reply.started":"2022-11-20T15:09:09.047186Z","shell.execute_reply":"2022-11-20T15:09:09.051029Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Dataset\n- Add [this dataset](https://www.kaggle.com/datasets/romainpessia/artificial-lunar-rocky-landscape-dataset) to your Kaggle notebook.\n\n## Instructions for this project\n- Don't forget to turn on GPU for training your model(s).\n- Without changing anything in the notebook if you run it then you will get the val_iou_score of around 0.20.\n- Your goal of this project is to increase this val_iou_score as much as you can.\n- Evaluation of this project will be based on your best acquired val_iou_score seen in the notebook.\n- Your val_iou_score will be the percentage you will receive for this project. \n- If your best val_iou_score is 0.41 then you will score 41/100 points in this project.\n- Try to avoid any errors before submitting your notebook.\n\n## Tips to increase the performance of your model\n- Increase the number of epochs.\n- Increase the number of layers in your model.\n- Using SOTA high performance networks with transfer learning.\n- Using callbacks and carefully observing your model performance.\n- You can use the methods taught to you in this training program or any other methods of your own choice to increase the performance!\n\n## Guidelines on making changes to this notebook\n**1)** Add a descriptive comment to your code for whatever changes you are making in this notebook.\n- For example, if you are adding an extra Conv2D layer, write about all the aspects of the Conv2D layer you are adding.\n- The commnt should be placed at the point where the layer will be added.\n\n**2)** Show model properties before and after the changes were made.\n- For example, if you changed the layers - added, deleted, e.t.c.\n\n**3)** If you use new data preprocessing techniques that are not already part of this notebook, you must explain their inner workings using markdown cells.\n- Without this explanation, your techniques will not be considered for evaluation.\n- Use texts and images to explain this process.\n\n**4)** Make use of tables and plots that contributed to the improvement of your model.\n- Assume if increasing the epochs and decreasing the learning rate contributed in the improvement of your model.\n- You will first show these improvements using plots of val_iou_scores vs epochs as well as val_iou_scores vs learning rate.0\n- Then make use of tables to show iou scores for different learning rates.\n- For example, table 1 for lr_1 to show iou values for epochs 30 t0 50, table 2 to show iou values from epochs 30 to 50, and so on.\n- It is therefore advised to work on one improvement, optimize it, plot it, document it, then proceed to the next improvement - till you get a satisfactory IOU score.\n\n**5)** Final improvement summary table.\n- Prepare a table with columns (changes, improvments description, increase in iou from, increase in iou to)\n- List out all the changes you made to improve your final model performance.","metadata":{"id":"_qWKqzF4415S"}},{"cell_type":"markdown","source":"## Coding for this project","metadata":{}},{"cell_type":"code","source":"!pip install -q segmentation_models","metadata":{"id":"HRJUKjN1QXgS","execution":{"iopub.status.busy":"2022-11-20T15:09:09.918433Z","iopub.execute_input":"2022-11-20T15:09:09.918793Z","iopub.status.idle":"2022-11-20T15:09:19.395894Z","shell.execute_reply.started":"2022-11-20T15:09:09.918761Z","shell.execute_reply":"2022-11-20T15:09:19.394674Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# import the necessary Library\nimport tensorflow as tf\nimport segmentation_models as sm\nimport glob\nimport cv2\nimport os\nimport keras\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt \nfrom sklearn.model_selection import train_test_split","metadata":{"id":"bGIcq3_DQXgT","execution":{"iopub.status.busy":"2022-11-20T15:09:19.398753Z","iopub.execute_input":"2022-11-20T15:09:19.399514Z","iopub.status.idle":"2022-11-20T15:09:19.408036Z","shell.execute_reply.started":"2022-11-20T15:09:19.399467Z","shell.execute_reply":"2022-11-20T15:09:19.406826Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"* Provide environment variable SM_FRAMEWORK=keras / SM_FRAMEWORK=tf.keras before import segmentation_models\n* Change framework sm.set_framework('keras') / sm.set_framework('tf.keras')","metadata":{"id":"TIJMgWvKm8y7"}},{"cell_type":"code","source":"# Setting framework environment\nos.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\nsm.set_framework('tf.keras')\nkeras.backend.set_image_data_format('channels_last')","metadata":{"id":"x2HKIVofm8y7","execution":{"iopub.status.busy":"2022-11-20T15:09:19.409686Z","iopub.execute_input":"2022-11-20T15:09:19.410285Z","iopub.status.idle":"2022-11-20T15:09:19.421179Z","shell.execute_reply.started":"2022-11-20T15:09:19.410248Z","shell.execute_reply":"2022-11-20T15:09:19.420119Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# To hide warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-11-20T15:09:19.424629Z","iopub.execute_input":"2022-11-20T15:09:19.425700Z","iopub.status.idle":"2022-11-20T15:09:19.431224Z","shell.execute_reply.started":"2022-11-20T15:09:19.425527Z","shell.execute_reply":"2022-11-20T15:09:19.430188Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing Pipeline","metadata":{"id":"OdS9NTtwQXgV"}},{"cell_type":"code","source":"H = 480 # height of image, has been changed to 480 from 256, since reducing the size of the original image would result in lower amount of features for the model to train on, hence a lower score.\nW = 480 # width of image, has been changed to 480 from 256, since reducing the size of the original image would result in lower amount of features for the model to train on, hence a lower score. Could not be able to be changed to 720 since VGG16 models accept only square shaped images as input.\n\n'''This function is used to return the list of path for images and masks in\nsorted order from the given directory respectively.'''\n# function to return list of image paths and mask paths \ndef process_data(IMG_DIR, MASK_DIR):\n    images = [os.path.join(IMG_DIR, x) for x in sorted(os.listdir(IMG_DIR))]\n    masks = [os.path.join(MASK_DIR, x) for x in sorted(os.listdir(MASK_DIR))]\n\n    return images, masks\n\n'''This function is used to return splitted list of images and corresponding \nmask paths in train and test by providing test size.'''\n# function to load data and train test split\ndef load_data(IMG_DIR, MASK_DIR):\n    X, y = process_data(IMG_DIR, MASK_DIR)\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42)\n    \n    return X_train, X_test, y_train, y_test\n\n'''This function is used to read images. It takes image path as input. \nAfter reading image it is resized by width and height provide above(256 x 256). \nNext normalization is done by dividing each values with 255. And the result is returned.'''\n# function to read image\ndef read_image(x):\n    x = cv2.imread(x, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (W, H))\n    x = x / 255.0\n    x = x.astype(np.float32)\n    return x\n\n'''This function is used to read masks.'''\n# function to read mask\ndef read_mask(x):\n    x = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, (W, H))\n    x = x.astype(np.int32)\n    return x\n\n'''This function is used to generate tensorflow data pipeline. \nThe tensorflow data pipeline is mapped to function ‘preprocess’ .'''\n# function for tensorflow dataset pipeline\ndef tf_dataset(x, y, batch=8):\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n    dataset = dataset.shuffle(buffer_size=5000)\n    dataset = dataset.map(preprocess)\n    dataset = dataset.batch(batch)\n    dataset = dataset.repeat()\n    dataset = dataset.prefetch(2)\n    return dataset\n\n'''This function takes image and mask path. \nIt reads the image and mask as provided by paths. \nMask is one hot encoded for multi class segmentation (here 4 class).'''\n# function to read image and mask amd create one hot encoding for mask\ndef preprocess(x, y):\n    def f(x, y):\n        x = x.decode()\n        y = y.decode()\n\n        image = read_image(x)\n        mask = read_mask(y)\n\n        return image, mask\n\n    image, mask = tf.numpy_function(f, [x, y], [tf.float32, tf.int32])\n    mask = tf.one_hot(mask, 4, dtype=tf.int32)\n    image.set_shape([H, W, 3])\n    mask.set_shape([H, W, 4])\n\n    return image, mask","metadata":{"id":"EQGsLbOVQXgW","execution":{"iopub.status.busy":"2022-11-20T15:09:19.824264Z","iopub.execute_input":"2022-11-20T15:09:19.824824Z","iopub.status.idle":"2022-11-20T15:09:19.847753Z","shell.execute_reply.started":"2022-11-20T15:09:19.824789Z","shell.execute_reply":"2022-11-20T15:09:19.846433Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"## Load the dataset","metadata":{"id":"ufOlyg7MQXgY"}},{"cell_type":"code","source":"'''RENDER_IMAGE_DIR_PATH: ‘Path of image directory’\nGROUND_MASK_DIR_PATH: ‘Path of mask directory’\n\nHere load_data function is called. This will load the dataset paths and \nsplit it into X_train, X_test, y_train, y_test '''\n\nRENDER_IMAGE_DIR_PATH = '../input/artificial-lunar-rocky-landscape-dataset/images/render'\nGROUND_MASK_DIR_PATH = '../input/artificial-lunar-rocky-landscape-dataset/images/clean'\n\nX_train, X_test, y_train, y_test = load_data(RENDER_IMAGE_DIR_PATH, GROUND_MASK_DIR_PATH)\nprint(f\"Dataset:\\n Train: {len(X_train)} \\n Test: {len(X_test)}\")","metadata":{"id":"vHWstFNTQXgY","execution":{"iopub.status.busy":"2022-11-20T15:09:19.850639Z","iopub.execute_input":"2022-11-20T15:09:19.852024Z","iopub.status.idle":"2022-11-20T15:09:19.997586Z","shell.execute_reply.started":"2022-11-20T15:09:19.851985Z","shell.execute_reply":"2022-11-20T15:09:19.996624Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Dataset:\n Train: 7812 \n Test: 1954\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Generate tensorflow data pipeline","metadata":{"id":"WfSTVsjCQXgZ"}},{"cell_type":"code","source":"batch_size = 16 # Changed the batch size to 16 to match the hyperparameter provided to the model\n\n'''Here the tf_dataset function is called will generate the tensorflow data pipeline.'''\n# calling tf_dataset\ntrain_dataset = tf_dataset(X_train, y_train, batch=batch_size)\nvalid_dataset = tf_dataset(X_test, y_test, batch=batch_size)","metadata":{"id":"4xsJKtW0QXgZ","execution":{"iopub.status.busy":"2022-11-20T15:09:19.999312Z","iopub.execute_input":"2022-11-20T15:09:20.000095Z","iopub.status.idle":"2022-11-20T15:09:20.162260Z","shell.execute_reply.started":"2022-11-20T15:09:20.000058Z","shell.execute_reply":"2022-11-20T15:09:20.161230Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"## Creating U-net Architecture","metadata":{"id":"1MqxtDTmQXga"}},{"cell_type":"code","source":"# Creating U-Net Architecture from scratch was resulting in lower val_iou_socres\n\n# from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, UpSampling2D, Concatenate\n# from tensorflow.keras.models import Model\n\n# '''conv_block it is used to create one block with two convolution layer \n# followed by BatchNormalization and activation function relu. \n# If the pooling is required then Maxpool2D is applied and return it else not.'''\n# # function to create convolution block\n# def conv_block(inputs, filters, pool=True):\n#     x = Conv2D(filters, 3, padding=\"same\")(inputs)\n#     x = BatchNormalization()(x)\n#     x = Activation(\"relu\")(x)\n\n#     x = Conv2D(filters, 3, padding=\"same\")(x)\n#     x = BatchNormalization()(x)\n#     x = Activation(\"relu\")(x)\n\n#     if pool == True:\n#         p = MaxPool2D((2, 2))(x)\n#         return x, p\n#     else:\n#         return x\n\n# '''build_unet it is used to create the U-net architecture.'''\n# # function to build U-net\n# def build_unet(shape, num_classes):\n#     inputs = Input(shape)\n\n#     \"\"\" Encoder \"\"\"\n#     x1, p1 = conv_block(inputs, 16, pool=True)\n#     x2, p2 = conv_block(p1, 32, pool=True)\n#     x3, p3 = conv_block(p2, 48, pool=True)\n#     x4, p4 = conv_block(p3, 64, pool=True)\n\n#     \"\"\" Bridge \"\"\"\n#     b1 = conv_block(p4, 128, pool=False)\n\n#     \"\"\" Decoder \"\"\"\n#     u1 = UpSampling2D((2, 2), interpolation=\"bilinear\")(b1)\n#     c1 = Concatenate()([u1, x4])\n#     x5 = conv_block(c1, 64, pool=False)\n\n#     u2 = UpSampling2D((2, 2), interpolation=\"bilinear\")(x5)\n#     c2 = Concatenate()([u2, x3])\n#     x6 = conv_block(c2, 48, pool=False)\n\n#     u3 = UpSampling2D((2, 2), interpolation=\"bilinear\")(x6)\n#     c3 = Concatenate()([u3, x2])\n#     x7 = conv_block(c3, 32, pool=False)\n\n#     u4 = UpSampling2D((2, 2), interpolation=\"bilinear\")(x7)\n#     c4 = Concatenate()([u4, x1])\n#     x8 = conv_block(c4, 16, pool=False)\n\n#     \"\"\" Output layer \"\"\"\n#     output = Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(x8)\n\n#     return Model(inputs, output)","metadata":{"id":"tYCyf8smQXga","execution":{"iopub.status.busy":"2022-11-20T15:09:20.164278Z","iopub.execute_input":"2022-11-20T15:09:20.165173Z","iopub.status.idle":"2022-11-20T15:09:20.177184Z","shell.execute_reply.started":"2022-11-20T15:09:20.165110Z","shell.execute_reply":"2022-11-20T15:09:20.175440Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"## Load model and compile","metadata":{"id":"bMgeqmX2QXgc"}},{"cell_type":"code","source":"# importing libraries\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\nfrom segmentation_models.metrics import iou_score\nimport datetime, os\n\n\"\"\" Defining Hyperparameters \"\"\"\nimg_shape = (480, 480, 3)\nnum_classes = 4\nlr = 5e-5\nbatch_size = 16\nepochs = 10\n# Parameters for VGG16 Pretrained Model\nBACKBONE = 'vgg16'\nactivation = 'softmax'\n\n\n\"\"\" Model building and compiling \"\"\"\n# Metrics for VGG16 Pretrained Model\nmetrics = [sm.metrics.IOUScore(threshold=0.5)]\n\n# Building a UNet with VGG16 as Backbone using segmetation_models library\nmodel = sm.Unet(backbone_name = BACKBONE, \n                input_shape = img_shape, \n                classes = num_classes, \n                activation = activation,\n                encoder_weights = 'imagenet')\nmodel.summary()\n\n# Using RMSprop optimizer for stabilizing the val_iou_score in each epoch. \nmodel.compile(loss = 'categorical_crossentropy', \n              optimizer = tf.keras.optimizers.RMSprop(lr), \n              metrics = metrics)\n\ntrain_steps = len(X_train)//batch_size\nvalid_steps = len(X_test)//batch_size","metadata":{"id":"z91qV2ZwQXgc","execution":{"iopub.status.busy":"2022-11-20T15:09:20.189359Z","iopub.execute_input":"2022-11-20T15:09:20.190205Z","iopub.status.idle":"2022-11-20T15:09:20.854345Z","shell.execute_reply.started":"2022-11-20T15:09:20.190170Z","shell.execute_reply":"2022-11-20T15:09:20.853321Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Model: \"model_11\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_12 (InputLayer)           [(None, 480, 480, 3) 0                                            \n__________________________________________________________________________________________________\nblock1_conv1 (Conv2D)           (None, 480, 480, 64) 1792        input_12[0][0]                   \n__________________________________________________________________________________________________\nblock1_conv2 (Conv2D)           (None, 480, 480, 64) 36928       block1_conv1[0][0]               \n__________________________________________________________________________________________________\nblock1_pool (MaxPooling2D)      (None, 240, 240, 64) 0           block1_conv2[0][0]               \n__________________________________________________________________________________________________\nblock2_conv1 (Conv2D)           (None, 240, 240, 128 73856       block1_pool[0][0]                \n__________________________________________________________________________________________________\nblock2_conv2 (Conv2D)           (None, 240, 240, 128 147584      block2_conv1[0][0]               \n__________________________________________________________________________________________________\nblock2_pool (MaxPooling2D)      (None, 120, 120, 128 0           block2_conv2[0][0]               \n__________________________________________________________________________________________________\nblock3_conv1 (Conv2D)           (None, 120, 120, 256 295168      block2_pool[0][0]                \n__________________________________________________________________________________________________\nblock3_conv2 (Conv2D)           (None, 120, 120, 256 590080      block3_conv1[0][0]               \n__________________________________________________________________________________________________\nblock3_conv3 (Conv2D)           (None, 120, 120, 256 590080      block3_conv2[0][0]               \n__________________________________________________________________________________________________\nblock3_pool (MaxPooling2D)      (None, 60, 60, 256)  0           block3_conv3[0][0]               \n__________________________________________________________________________________________________\nblock4_conv1 (Conv2D)           (None, 60, 60, 512)  1180160     block3_pool[0][0]                \n__________________________________________________________________________________________________\nblock4_conv2 (Conv2D)           (None, 60, 60, 512)  2359808     block4_conv1[0][0]               \n__________________________________________________________________________________________________\nblock4_conv3 (Conv2D)           (None, 60, 60, 512)  2359808     block4_conv2[0][0]               \n__________________________________________________________________________________________________\nblock4_pool (MaxPooling2D)      (None, 30, 30, 512)  0           block4_conv3[0][0]               \n__________________________________________________________________________________________________\nblock5_conv1 (Conv2D)           (None, 30, 30, 512)  2359808     block4_pool[0][0]                \n__________________________________________________________________________________________________\nblock5_conv2 (Conv2D)           (None, 30, 30, 512)  2359808     block5_conv1[0][0]               \n__________________________________________________________________________________________________\nblock5_conv3 (Conv2D)           (None, 30, 30, 512)  2359808     block5_conv2[0][0]               \n__________________________________________________________________________________________________\nblock5_pool (MaxPooling2D)      (None, 15, 15, 512)  0           block5_conv3[0][0]               \n__________________________________________________________________________________________________\ncenter_block1_conv (Conv2D)     (None, 15, 15, 512)  2359296     block5_pool[0][0]                \n__________________________________________________________________________________________________\ncenter_block1_bn (BatchNormaliz (None, 15, 15, 512)  2048        center_block1_conv[0][0]         \n__________________________________________________________________________________________________\ncenter_block1_relu (Activation) (None, 15, 15, 512)  0           center_block1_bn[0][0]           \n__________________________________________________________________________________________________\ncenter_block2_conv (Conv2D)     (None, 15, 15, 512)  2359296     center_block1_relu[0][0]         \n__________________________________________________________________________________________________\ncenter_block2_bn (BatchNormaliz (None, 15, 15, 512)  2048        center_block2_conv[0][0]         \n__________________________________________________________________________________________________\ncenter_block2_relu (Activation) (None, 15, 15, 512)  0           center_block2_bn[0][0]           \n__________________________________________________________________________________________________\ndecoder_stage0_upsampling (UpSa (None, 30, 30, 512)  0           center_block2_relu[0][0]         \n__________________________________________________________________________________________________\ndecoder_stage0_concat (Concaten (None, 30, 30, 1024) 0           decoder_stage0_upsampling[0][0]  \n                                                                 block5_conv3[0][0]               \n__________________________________________________________________________________________________\ndecoder_stage0a_conv (Conv2D)   (None, 30, 30, 256)  2359296     decoder_stage0_concat[0][0]      \n__________________________________________________________________________________________________\ndecoder_stage0a_bn (BatchNormal (None, 30, 30, 256)  1024        decoder_stage0a_conv[0][0]       \n__________________________________________________________________________________________________\ndecoder_stage0a_relu (Activatio (None, 30, 30, 256)  0           decoder_stage0a_bn[0][0]         \n__________________________________________________________________________________________________\ndecoder_stage0b_conv (Conv2D)   (None, 30, 30, 256)  589824      decoder_stage0a_relu[0][0]       \n__________________________________________________________________________________________________\ndecoder_stage0b_bn (BatchNormal (None, 30, 30, 256)  1024        decoder_stage0b_conv[0][0]       \n__________________________________________________________________________________________________\ndecoder_stage0b_relu (Activatio (None, 30, 30, 256)  0           decoder_stage0b_bn[0][0]         \n__________________________________________________________________________________________________\ndecoder_stage1_upsampling (UpSa (None, 60, 60, 256)  0           decoder_stage0b_relu[0][0]       \n__________________________________________________________________________________________________\ndecoder_stage1_concat (Concaten (None, 60, 60, 768)  0           decoder_stage1_upsampling[0][0]  \n                                                                 block4_conv3[0][0]               \n__________________________________________________________________________________________________\ndecoder_stage1a_conv (Conv2D)   (None, 60, 60, 128)  884736      decoder_stage1_concat[0][0]      \n__________________________________________________________________________________________________\ndecoder_stage1a_bn (BatchNormal (None, 60, 60, 128)  512         decoder_stage1a_conv[0][0]       \n__________________________________________________________________________________________________\ndecoder_stage1a_relu (Activatio (None, 60, 60, 128)  0           decoder_stage1a_bn[0][0]         \n__________________________________________________________________________________________________\ndecoder_stage1b_conv (Conv2D)   (None, 60, 60, 128)  147456      decoder_stage1a_relu[0][0]       \n__________________________________________________________________________________________________\ndecoder_stage1b_bn (BatchNormal (None, 60, 60, 128)  512         decoder_stage1b_conv[0][0]       \n__________________________________________________________________________________________________\ndecoder_stage1b_relu (Activatio (None, 60, 60, 128)  0           decoder_stage1b_bn[0][0]         \n__________________________________________________________________________________________________\ndecoder_stage2_upsampling (UpSa (None, 120, 120, 128 0           decoder_stage1b_relu[0][0]       \n__________________________________________________________________________________________________\ndecoder_stage2_concat (Concaten (None, 120, 120, 384 0           decoder_stage2_upsampling[0][0]  \n                                                                 block3_conv3[0][0]               \n__________________________________________________________________________________________________\ndecoder_stage2a_conv (Conv2D)   (None, 120, 120, 64) 221184      decoder_stage2_concat[0][0]      \n__________________________________________________________________________________________________\ndecoder_stage2a_bn (BatchNormal (None, 120, 120, 64) 256         decoder_stage2a_conv[0][0]       \n__________________________________________________________________________________________________\ndecoder_stage2a_relu (Activatio (None, 120, 120, 64) 0           decoder_stage2a_bn[0][0]         \n__________________________________________________________________________________________________\ndecoder_stage2b_conv (Conv2D)   (None, 120, 120, 64) 36864       decoder_stage2a_relu[0][0]       \n__________________________________________________________________________________________________\ndecoder_stage2b_bn (BatchNormal (None, 120, 120, 64) 256         decoder_stage2b_conv[0][0]       \n__________________________________________________________________________________________________\ndecoder_stage2b_relu (Activatio (None, 120, 120, 64) 0           decoder_stage2b_bn[0][0]         \n__________________________________________________________________________________________________\ndecoder_stage3_upsampling (UpSa (None, 240, 240, 64) 0           decoder_stage2b_relu[0][0]       \n__________________________________________________________________________________________________\ndecoder_stage3_concat (Concaten (None, 240, 240, 192 0           decoder_stage3_upsampling[0][0]  \n                                                                 block2_conv2[0][0]               \n__________________________________________________________________________________________________\ndecoder_stage3a_conv (Conv2D)   (None, 240, 240, 32) 55296       decoder_stage3_concat[0][0]      \n__________________________________________________________________________________________________\ndecoder_stage3a_bn (BatchNormal (None, 240, 240, 32) 128         decoder_stage3a_conv[0][0]       \n__________________________________________________________________________________________________\ndecoder_stage3a_relu (Activatio (None, 240, 240, 32) 0           decoder_stage3a_bn[0][0]         \n__________________________________________________________________________________________________\ndecoder_stage3b_conv (Conv2D)   (None, 240, 240, 32) 9216        decoder_stage3a_relu[0][0]       \n__________________________________________________________________________________________________\ndecoder_stage3b_bn (BatchNormal (None, 240, 240, 32) 128         decoder_stage3b_conv[0][0]       \n__________________________________________________________________________________________________\ndecoder_stage3b_relu (Activatio (None, 240, 240, 32) 0           decoder_stage3b_bn[0][0]         \n__________________________________________________________________________________________________\ndecoder_stage4_upsampling (UpSa (None, 480, 480, 32) 0           decoder_stage3b_relu[0][0]       \n__________________________________________________________________________________________________\ndecoder_stage4a_conv (Conv2D)   (None, 480, 480, 16) 4608        decoder_stage4_upsampling[0][0]  \n__________________________________________________________________________________________________\ndecoder_stage4a_bn (BatchNormal (None, 480, 480, 16) 64          decoder_stage4a_conv[0][0]       \n__________________________________________________________________________________________________\ndecoder_stage4a_relu (Activatio (None, 480, 480, 16) 0           decoder_stage4a_bn[0][0]         \n__________________________________________________________________________________________________\ndecoder_stage4b_conv (Conv2D)   (None, 480, 480, 16) 2304        decoder_stage4a_relu[0][0]       \n__________________________________________________________________________________________________\ndecoder_stage4b_bn (BatchNormal (None, 480, 480, 16) 64          decoder_stage4b_conv[0][0]       \n__________________________________________________________________________________________________\ndecoder_stage4b_relu (Activatio (None, 480, 480, 16) 0           decoder_stage4b_bn[0][0]         \n__________________________________________________________________________________________________\nfinal_conv (Conv2D)             (None, 480, 480, 4)  580         decoder_stage4b_relu[0][0]       \n__________________________________________________________________________________________________\nsoftmax (Activation)            (None, 480, 480, 4)  0           final_conv[0][0]                 \n==================================================================================================\nTotal params: 23,752,708\nTrainable params: 23,748,676\nNon-trainable params: 4,032\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train model","metadata":{"id":"SBhRPBKPQXgc"}},{"cell_type":"code","source":"'''model.fit is used to train the model'''\nmodel_history = model.fit(train_dataset,\n        epochs = epochs,\n        steps_per_epoch = train_steps,\n        validation_data = valid_dataset,\n        validation_steps = valid_steps\n)","metadata":{"id":"4lJgBNVwQXgd","execution":{"iopub.status.busy":"2022-11-20T15:09:20.855732Z","iopub.execute_input":"2022-11-20T15:09:20.856543Z","iopub.status.idle":"2022-11-20T16:14:46.368816Z","shell.execute_reply.started":"2022-11-20T15:09:20.856505Z","shell.execute_reply":"2022-11-20T16:14:46.367568Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Epoch 1/10\n488/488 [==============================] - 397s 807ms/step - loss: 0.2731 - iou_score: 0.2180 - val_loss: 0.1102 - val_iou_score: 0.3769\nEpoch 2/10\n488/488 [==============================] - 392s 804ms/step - loss: 0.0596 - iou_score: 0.7209 - val_loss: 0.0288 - val_iou_score: 0.6923\nEpoch 3/10\n488/488 [==============================] - 393s 804ms/step - loss: 0.0204 - iou_score: 0.9309 - val_loss: 0.0120 - val_iou_score: 0.9401\nEpoch 4/10\n488/488 [==============================] - 391s 802ms/step - loss: 0.0117 - iou_score: 0.9404 - val_loss: 0.0110 - val_iou_score: 0.9401\nEpoch 5/10\n488/488 [==============================] - 391s 802ms/step - loss: 0.0082 - iou_score: 0.9397 - val_loss: 0.0055 - val_iou_score: 0.9398\nEpoch 6/10\n488/488 [==============================] - 391s 802ms/step - loss: 0.0055 - iou_score: 0.9401 - val_loss: 0.0051 - val_iou_score: 0.9398\nEpoch 7/10\n488/488 [==============================] - 392s 804ms/step - loss: 0.0045 - iou_score: 0.9400 - val_loss: 0.0050 - val_iou_score: 0.9397\nEpoch 8/10\n488/488 [==============================] - 392s 803ms/step - loss: 0.0054 - iou_score: 0.9400 - val_loss: 0.0059 - val_iou_score: 0.9397\nEpoch 9/10\n488/488 [==============================] - 391s 802ms/step - loss: 0.0038 - iou_score: 0.9400 - val_loss: 0.0032 - val_iou_score: 0.9398\nEpoch 10/10\n488/488 [==============================] - 392s 803ms/step - loss: 0.0028 - iou_score: 0.9400 - val_loss: 0.0039 - val_iou_score: 0.9397\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Final Model History\")\nprint(model_history.history)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T16:14:49.317681Z","iopub.execute_input":"2022-11-20T16:14:49.318030Z","iopub.status.idle":"2022-11-20T16:14:49.324228Z","shell.execute_reply.started":"2022-11-20T16:14:49.318000Z","shell.execute_reply":"2022-11-20T16:14:49.323286Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Final Model History\n{'loss': [0.2730637490749359, 0.05958528816699982, 0.02043072134256363, 0.011736622080206871, 0.008204606361687183, 0.005505457986146212, 0.004455679561942816, 0.005366102326661348, 0.003807353088632226, 0.0028437115252017975], 'iou_score': [0.21798183023929596, 0.7209295630455017, 0.9309038519859314, 0.940369188785553, 0.9396529197692871, 0.9401271939277649, 0.940005362033844, 0.9400326609611511, 0.9400274157524109, 0.9400460720062256], 'val_loss': [0.11020644754171371, 0.028828931972384453, 0.011978352442383766, 0.011032418347895145, 0.005451821256428957, 0.005057929549366236, 0.00495494669303298, 0.005906953476369381, 0.003229364985600114, 0.0038589152973145247], 'val_iou_score': [0.3768904209136963, 0.6922722458839417, 0.9400937557220459, 0.9401147365570068, 0.9397609829902649, 0.9397558569908142, 0.9397373795509338, 0.9397491216659546, 0.9397779703140259, 0.9397419691085815]}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## [IMPORTANT] Paste you final model training history here in the markdown.(just double click this line, and you'll be able to edit it. \n\nNOTE: If we find that your actual model score and what you paste here is differing, your assignment will get rejected.  \n\nFinal Model History  \n{'loss': [0.2730637490749359, 0.05958528816699982, 0.02043072134256363, 0.011736622080206871, 0.008204606361687183, 0.005505457986146212, 0.004455679561942816, 0.005366102326661348, 0.003807353088632226, 0.0028437115252017975], 'iou_score': [0.21798183023929596, 0.7209295630455017, 0.9309038519859314, 0.940369188785553, 0.9396529197692871, 0.9401271939277649, 0.940005362033844, 0.9400326609611511, 0.9400274157524109, 0.9400460720062256], 'val_loss': [0.11020644754171371, 0.028828931972384453, 0.011978352442383766, 0.011032418347895145, 0.005451821256428957, 0.005057929549366236, 0.00495494669303298, 0.005906953476369381, 0.003229364985600114, 0.0038589152973145247], 'val_iou_score': [0.3768904209136963, 0.6922722458839417, 0.9400937557220459, 0.9401147365570068, 0.9397609829902649, 0.9397558569908142, 0.9397373795509338, 0.9397491216659546, 0.9397779703140259, 0.9397419691085815]}\n\n","metadata":{"id":"I1OFxBbnxC_I"}},{"cell_type":"markdown","source":"# Atempts and Descriptions\n| SNo.        | Description                              | Previous val_iou_score | Current val_iou_score |\n| ----------- | -----------                              | -----------------------|-----------------------|\n| 1           | No Changes made to any parameters!       |      -                 |    0.196758           |\n| 2           | Noticed that the original shape of the image is 480 x 720, but the original notebook was resizing the image to 256 x 256.By reducing the size of the image, many features in the image are lost. This is why I changed the height and width of the image to the original image's height and width.| 0.196758 | 0.194496 | \n| 3           | Since the number of epochs is too low, tried increasing the number of epochs to 10 to check for improvement.|0.194496                |    0.199426           |\n| 4           | Decreasing the batch size to 8 in order to have the model train on more images on each epoch. Increased the number of epochs to 20 to check the effect. |  0.199426                |    0.193903           |\n| 5           | Since the traditional method of manually creating tensorflow layers and building a UNet seems to result in lower val_iou_scores, we can try incorporate Trasfer learning using the VGG16 pretrained model.Since the VGG16 model accepts only square images as input, input image shape was changed to (480, 480, 3)|      0.193903                 |    0.232177           |\n| 6           | Adjusting the learning rate in order to lower the fluctuation in val_iou_score.       |      0.232177                |    0.690185           |\n| 7           | Adjusting the batch size to 16 to reduce overfitting.      |      0.690185                 |    0.917532           |\n| 8           | Changing the optimizer to RMSprop to try to control the fluctuations in the model's val_iou_score       |      0.917532               |    0.939769           |\n| 9           | Trying to decrease the number of epochs to check if model stabilizes at 0.939 val_iou_socre.       |      0.939769                |     0.939742           |\n","metadata":{}},{"cell_type":"markdown","source":"---\n# <center> THE END","metadata":{}}]}
